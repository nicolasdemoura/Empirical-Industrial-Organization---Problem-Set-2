# Empirical Industrial Organization - Problem Set 2

## Estimating Incomplete Information Dynamic Games: A Monte Carlo Comparison

**Authors:** N√≠colas de Moura ([nicolasgoulartdemoura@gmail.com](mailto:nicolasgoulartdemoura@gmail.com)) and Michel Wachsmann ([michel@wachsmann@gmail.com](mailto:michel@wachsmann@gmail.com))

**Date:** December 2025

---

## Overview

This project implements and compares two structural estimation methods for dynamic games with incomplete information:

1. **Asymptotic Least Squares (ALS)** - Pesendorfer and Schmidt-Dengler (2008)
2. **Forward Simulation Method** - Hotz, Miller, Sanders, and Smith (1994)

The code conducts extensive Monte Carlo simulations to evaluate the finite-sample performance of these estimators across different sample sizes and simulation horizons.

## Project Structure

```
.
‚îú‚îÄ‚îÄ main.R                    # Main execution script with Monte Carlo simulations
‚îú‚îÄ‚îÄ model.R                   # Game setup, equilibrium solver, and core model functions
‚îú‚îÄ‚îÄ simulation.R              # Data generation from the dynamic game
‚îú‚îÄ‚îÄ estimation.R              # ALS estimator implementation
‚îú‚îÄ‚îÄ hotz_miller.R            # Forward simulation method (HMSS 1994)
‚îú‚îÄ‚îÄ performance.R            # Performance metrics and evaluation functions
‚îú‚îÄ‚îÄ markov_chain_viz.R       # Visualization utilities for Markov chains
‚îî‚îÄ‚îÄ output/                  # Simulation results and tables
    ‚îú‚îÄ‚îÄ *.rds                # Saved R data objects
    ‚îî‚îÄ‚îÄ *.csv                # Summary tables
```

## Model Description

### Dynamic Game Framework

The model implements a two-player dynamic game with:

- **Players:** 2 firms (symmetric)
- **Actions:** Binary choice {0, 1} (inactive/active)
- **States:** Market configurations based on past actions
- **Transition:** Deterministic (next state = current action profile)
- **Discount Factor:** Œ≤ = 0.9

### Payoff Structure

For player *i* with action *a·µ¢* in state *s*:

```
œÄ(a·µ¢, a‚Çã·µ¢, s·µ¢) = a·µ¢ √ó [œÄ‚ÇÄ(1 - a‚Çã·µ¢) + œÄ‚ÇÅa‚Çã·µ¢] + a·µ¢(1 - s·µ¢)F + (1 - a·µ¢)s·µ¢W
```

Where:
- **œÄ‚ÇÄ = 1.2:** Monopoly payoff (opponent inactive)
- **œÄ‚ÇÅ = -1.2:** Duopoly payoff (opponent active)
- **F = -0.2:** Fixed cost of entry
- **W = 0.1:** Scrap value from exit

### Private Information

Each player observes an i.i.d. type-I extreme value shock Œµ·µ¢‚Çê for each action, leading to logit-form conditional choice probabilities.

## Estimation Methods

### 1. Asymptotic Least Squares (PSD 2008)

The ALS estimator finds parameters Œ∏ by solving:

```
min Q(Œ∏) = Œ£‚Çõ Œ£‚Çê Œ£·µ¢ [BR_i(a|s; pÃÇ, Œ∏) - pÃÇ·µ¢(a|s)]¬≤ W(s,a)
```

**Key insight:** Observed choice probabilities pÃÇ should equal best responses under the estimated parameters.

**Implementations:**
- **Calibrated Œ≤:** Fix discount factor at true value (Œ≤ = 0.9)
- **Estimated Œ≤:** Jointly estimate all four parameters

### 2. Forward Simulation (HMSS 1994)

Computes continuation values via forward simulation:

```
VÃÇ·µ¢(s) = ùîº[Œ£‚Çú Œ≤·µóœÄ(a·µ¢‚Çú, a‚Çã·µ¢‚Çú, s‚Çú) | s‚ÇÄ = s, pÃÇ]
```

**Procedure:**
1. Estimate conditional choice probabilities (CCPs) from data
2. Simulate H forward periods from each state using CCPs
3. Average discounted payoffs over R simulation draws
4. Solve for parameters that rationalize observed CCPs

**Simulation Parameters:**
- **H:** Horizon lengths tested: {50, 100, 200, 500}
- **R:** Number of draws per state: 500 (parallelized)

## Monte Carlo Simulations

### Experimental Design

- **Sample Sizes (T):** 100, 1,000, 10,000, 100,000 periods
- **Monte Carlo Draws:** 1,000 repetitions per configuration
- **Initial State:** State 1 (both firms inactive)
- **Parallelization:** Multi-session future backend for computational efficiency

### Performance Metrics

For each estimator, we compute:
- **Bias:** Mean estimate - true value
- **Standard Error:** Standard deviation of estimates
- **Mean Squared Error (MSE):** Bias¬≤ + Variance
- **Convergence Rate:** Proportion of successful optimizations

## Dependencies

### Required R Packages

```r
install.packages(c(
  "dplyr",          # Data manipulation
  "ggplot2",        # Visualization
  "data.table",     # Fast data operations
  "MASS",           # Matrix operations
  "nleqslv",        # Nonlinear equation solver
  "progress",       # Progress bars
  "future.apply",   # Parallel processing
  "progressr",      # Advanced progress reporting
  "reshape2"        # Data reshaping
))
```

All dependencies are automatically checked and installed by `main.R`.

## Usage

### Running the Full Monte Carlo Study

```r
source("main.R")
```

This will:
1. Set up the game environment with true parameters
2. Solve for the equilibrium conditional choice probabilities
3. Run Monte Carlo simulations for all sample sizes
4. Apply both ALS and forward simulation estimators
5. Save results to `output/` directory
6. Generate performance comparison tables

### Running Individual Components

```r
# Load model functions
source("model.R")

# Set up game with custom parameters
game <- get_game_parameters(pi_0 = 1.2, pi_1 = -1.2, F = -0.2, beta = 0.9)

# Solve for equilibrium
equilibrium <- solve_conditional_action_probabilities(game, initial_beliefs)

# Simulate data
data <- simulate_game_data(game, draws = 100, num_periods = 1000, 
                          initial_state = 1, equilibrium = equilibrium)

# Estimate using ALS
source("estimation.R")
results <- estimate_parameters_als(data, initial_params = c(0, 0, 0), 
                                   estimate_beta = FALSE, beta_fixed = 0.9)

# Estimate using forward simulation
source("hotz_miller.R")
results_hm <- estimate_parameters_hotz_miller(data, H = 100, estimate_beta = FALSE)
```

### Output Files

Results are saved in the `output/` directory:

- **`PSD2008_Calibrated_Beta_T*.rds`:** ALS estimates with fixed Œ≤
- **`PSD2008_Estimated_Beta_T*.rds`:** ALS estimates with estimated Œ≤
- **`HMSS1994_H*_T*.rds`:** Forward simulation estimates
- **`comparison_*.rds`:** Head-to-head comparisons
- **`summary_comparison_table.csv`:** Aggregate performance metrics

## Key Findings

### Computational Efficiency

- **ALS:** Fast convergence, no simulation required
- **Forward Simulation:** Computationally intensive, requires H √ó R function evaluations

### Finite-Sample Performance

Performance varies by:
- Sample size T (larger is better)
- Simulation horizon H (forward simulation only)
- Parameter identification strength

See generated CSV tables in `output/` for detailed numerical results.

## Theoretical References

1. **Pesendorfer, M., & Schmidt-Dengler, P. (2008).** "Asymptotic Least Squares Estimators for Dynamic Games." *Econometrica*, 76(3), 503-523.

2. **Hotz, V. J., Miller, R. A., Sanders, S., & Smith, J. (1994).** "A Simulation Estimator for Dynamic Models of Discrete Choice." *The Review of Economic Studies*, 61(2), 265-289.

3. **Aguirregabiria, V., & Mira, P. (2007).** "Sequential Estimation of Dynamic Discrete Games." *Econometrica*, 75(1), 1-53.

## License

This project is for academic purposes as part of a graduate course in Empirical Industrial Organization.

## Contact

For questions or issues, please contact:
- N√≠colas de Moura: nicolasgoulartdemoura@gmail.com
- Michel Wachsmann: michel@wachsmann@gmail.com

---

**Last Updated:** December 10, 2025
